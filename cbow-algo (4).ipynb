{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T14:24:20.350463Z",
     "iopub.status.busy": "2023-02-03T14:24:20.349037Z",
     "iopub.status.idle": "2023-02-03T14:24:20.418997Z",
     "shell.execute_reply": "2023-02-03T14:24:20.417946Z",
     "shell.execute_reply.started": "2023-02-03T14:24:20.350397Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "\n",
    "def softmax(z):\n",
    "    e_z = np.exp(z)\n",
    "    sum_e_z = np.sum(e_z)\n",
    "    return e_z / sum_e_z\n",
    "\n",
    "class word2vec(object):\n",
    "    def __init__(self):\n",
    "        self.N = 300\n",
    "        self.X_train = []\n",
    "        self.y_train = []\n",
    "        self.window_size = 2\n",
    "        self.alpha = 0.003\n",
    "        self.words = []\n",
    "        self.word_index = {}\n",
    "        self.loss = 0\n",
    "\n",
    "    def initialize(self,V,data):\n",
    "        self.V = V\n",
    "        self.W1 = np.random.uniform(-1, 1, (self.N, self.V))\n",
    "        self.W2 = np.random.uniform(-1, 1, (self.V, self.N))\n",
    "        self.b1 = np.random.uniform(-1, 1, (self.N, 1))\n",
    "        self.b2 = np.random.uniform(-1, 1, (self.V, 1))\n",
    "\n",
    "          \n",
    "        self.words = data\n",
    "        for i in range(len(data)):\n",
    "            self.word_index[data[i]] = i\n",
    "  \n",
    "      \n",
    "    def feed_forward(self,X):\n",
    "#         print(X.shape)\n",
    "        X = X.reshape(self.V,1)\n",
    "        self.h = np.dot(self.W1,X) + self.b1\n",
    "        self.u = np.dot(self.W2 ,self.h) + self.b2\n",
    "        #print(self.u)\n",
    "        self.y_hat = softmax(self.u)\n",
    "        return self.y_hat\n",
    "          \n",
    "    def backpropagate(self,x,t):\n",
    "        t = t.reshape(self.V,1)\n",
    "        x = x.reshape(self.V,1)\n",
    "        self.grad_b2 = self.y_hat - t\n",
    "        self.grad_W2 = np.dot(self.y_hat - t, self.h.T)\n",
    "        self.grad_b1 = np.dot(self.W2.T, self.y_hat - t)\n",
    "        self.grad_W1 = np.dot(np.dot(self.W2.T, self.y_hat - t), x.T)\n",
    "        self.W1 = self.W1 - self.alpha * self.grad_W1\n",
    "        self.W2 = self.W2 - self.alpha * self.grad_W2\n",
    "        self.b1 = self.b1 - self.alpha * self.grad_b1\n",
    "        self.b2 = self.b2 - self.alpha * self.grad_b2\n",
    "#         return(self.W1_new , self.W2_new , self.b1_new, self.b2_new)\n",
    "\n",
    "\n",
    "    def train(self,epochs):\n",
    "        for x in range(1,epochs):       \n",
    "            self.loss = 0\n",
    "            for j in range(len(self.X_train)):\n",
    "                if(len(self.X_train[j])==0):\n",
    "                    continue\n",
    "#                 print(j)\n",
    "                self.y_hat = self.feed_forward(self.X_train[j])\n",
    "                self.backpropagate(self.X_train[j],self.y_train[j])\n",
    "                C = 0\n",
    "                for m in range(self.V):\n",
    "                    if(self.y_train[j][m]):\n",
    "                        self.loss += -1*self.u[m][0]\n",
    "                        C += 1\n",
    "                self.loss += C*np.log(np.sum(np.exp(self.u)))\n",
    "                if(j%1000==0):\n",
    "                    print(\"epoch \",x, \" loss = \",self.loss)\n",
    "            self.alpha *= 1/( (1+self.alpha*x) )\n",
    "             \n",
    "#     def predict(self,word,number_of_predictions):\n",
    "#         if word in self.words:\n",
    "#             index = self.word_index[word]\n",
    "#             X = [0 for i in range(self.V)]\n",
    "#             X[index] = 1\n",
    "#             prediction = self.feed_forward(X)\n",
    "#             output = {}\n",
    "#             for i in range(self.V):\n",
    "#                 output[prediction[i][0]] = i\n",
    "\n",
    "#             top_context_words = []\n",
    "#             for k in sorted(output,reverse=True):\n",
    "#                 top_context_words.append(self.words[output[k]])\n",
    "#                 if(len(top_context_words)>=number_of_predictions):\n",
    "#                     break\n",
    "      \n",
    "#             return top_context_words\n",
    "#         else:\n",
    "#             print(\"Word not found in dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T14:21:38.199472Z",
     "iopub.status.busy": "2023-02-03T14:21:38.198925Z",
     "iopub.status.idle": "2023-02-03T14:21:38.211800Z",
     "shell.execute_reply": "2023-02-03T14:21:38.210277Z",
     "shell.execute_reply.started": "2023-02-03T14:21:38.199422Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "    stop_words = set(stopwords.words('english'))   \n",
    "    training_data = []\n",
    "#     sentences = corpus.split(\".\")\n",
    "#     for i in range(len(sentences)):\n",
    "#         sentences[i] = sentences[i].strip()\n",
    "#         sentence = sentences[i].split()\n",
    "    x = [word.strip(string.punctuation) for word in sentence\n",
    "                                     if word not in stop_words]\n",
    "    x = [word for word in x if len(word)!=0]\n",
    "    x = [word.lower() for word in x]\n",
    "#     training_data.append(x)\n",
    "    return x\n",
    "\n",
    "def index_word_maps(data: list) -> tuple:\n",
    "    \n",
    "    words = sorted(list(set(data)))\n",
    "    \n",
    "    word_to_index = {word: index for index, word in enumerate(words)}\n",
    "    index_to_word = {index: word for index, word in enumerate(words)}\n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "\n",
    "def word_to_one_hot_vector(word: str, word_to_index: dict, vocabulary_size: int) -> np.ndarray:\n",
    "    \n",
    "    one_hot_vector = np.zeros(vocabulary_size)\n",
    "    one_hot_vector[word_to_index[word]] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "def context_words_to_vector(context_words: list,\n",
    "                            word_to_index: dict) -> np.ndarray:\n",
    "    vocabulary_size = len(word_to_index)\n",
    "    context_words_vectors = [\n",
    "        word_to_one_hot_vector(word, word_to_index, vocabulary_size)\n",
    "        for word in context_words]\n",
    "    return np.mean(context_words_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(corpus.split(\".\")[1])\n",
    "preprocessing(corpus)\n",
    "\n",
    "training_data = preprocessing(corpus)\n",
    "epochs = 3\n",
    "\n",
    "# X_train, Y_train = prepare_data_for_training(training_data,w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T14:21:44.853064Z",
     "iopub.status.busy": "2023-02-03T14:21:44.852561Z",
     "iopub.status.idle": "2023-02-03T14:21:50.076582Z",
     "shell.execute_reply": "2023-02-03T14:21:50.075232Z",
     "shell.execute_reply.started": "2023-02-03T14:21:44.853026Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "import nltk\n",
    "fields = nltk.corpus.gutenberg.fileids()\n",
    "\n",
    "words = []\n",
    "for field in fields:\n",
    "    words += list(gutenberg.words(field))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg.sents(\"shakespeare-macbeth.txt\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T14:21:54.794872Z",
     "iopub.status.busy": "2023-02-03T14:21:54.794448Z",
     "iopub.status.idle": "2023-02-03T14:21:55.854353Z",
     "shell.execute_reply": "2023-02-03T14:21:55.853098Z",
     "shell.execute_reply.started": "2023-02-03T14:21:54.794828Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english') + [\"[\",\"]\",\"''\"])\n",
    "words = [word.strip(string.punctuation) for word in words\n",
    "                                     if word not in stop_words and len(word)!=0]\n",
    "words = [word.lower() for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word = index_word_maps(words)\n",
    "word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T14:24:30.006924Z",
     "iopub.status.busy": "2023-02-03T14:24:30.005967Z",
     "iopub.status.idle": "2023-02-03T14:24:30.702580Z",
     "shell.execute_reply": "2023-02-03T14:24:30.701387Z",
     "shell.execute_reply.started": "2023-02-03T14:24:30.006857Z"
    }
   },
   "outputs": [],
   "source": [
    "w2v = word2vec()\n",
    "w2v.initialize(len(word_to_index.keys()),words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T14:24:35.260116Z",
     "iopub.status.busy": "2023-02-03T14:24:35.258856Z",
     "iopub.status.idle": "2023-02-03T14:24:47.607754Z",
     "shell.execute_reply": "2023-02-03T14:24:47.606841Z",
     "shell.execute_reply.started": "2023-02-03T14:24:35.260064Z"
    }
   },
   "outputs": [],
   "source": [
    "training_data = gutenberg.sents('shakespeare-macbeth.txt')\n",
    "\n",
    "for m in range(0,len(training_data)):\n",
    "    sent = training_data[m]\n",
    "    sent = preprocessing(sent)\n",
    "#     w2v.X_train = []\n",
    "#     w2v.y_train = []\n",
    "\n",
    "    for i in range(0,len(sent)):\n",
    "        context_words = []\n",
    "        train_word = word_to_one_hot_vector(sent[i],word_to_index,len(word_to_index.keys()))\n",
    "        for j in range(i-w2v.window_size,i+w2v.window_size):\n",
    "                if i!=j and j>=0 and j<len(sent):\n",
    "                    context_words.append(sent[j])\n",
    "        \n",
    "        if len(context_words) > 0:\n",
    "            w2v.y_train.append(train_word)\n",
    "            w2v.X_train.append(context_words_to_vector(context_words,word_to_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing(training_data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-03T14:25:06.863887Z",
     "iopub.status.busy": "2023-02-03T14:25:06.863364Z",
     "iopub.status.idle": "2023-02-03T15:08:40.480787Z",
     "shell.execute_reply": "2023-02-03T15:08:40.478560Z",
     "shell.execute_reply.started": "2023-02-03T14:25:06.863841Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  1  loss =  21.855757991734023\n",
      "epoch  1  loss =  23752.33259597954\n",
      "epoch  1  loss =  42723.44366991852\n",
      "epoch  1  loss =  60068.39626752783\n",
      "epoch  1  loss =  78027.99396048703\n",
      "epoch  1  loss =  95147.45370790405\n",
      "epoch  1  loss =  112496.25620092622\n",
      "epoch  1  loss =  129368.04053148233\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_929/322926949.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_929/1614898465.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m#                 print(j)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_929/1614898465.py\u001b[0m in \u001b[0;36mbackpropagate\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_b2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_hat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_W2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_hat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_b1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_hat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_W1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_hat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w2v.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2v.y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.X_train[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
